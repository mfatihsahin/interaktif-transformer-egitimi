(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[974],{503:(e,t,a)=>{"use strict";a.r(t),a.d(t,{default:()=>_});var s=a(5155),n=a(2115),r=a(226),i=a(6874),l=a.n(i);let o=[{name:"Introduction",href:"#introduction"},{name:"Sequence Models",href:"#seq-models"},{name:"Attention Mechanism",href:"#attention"},{name:"Transformer Architecture",href:"#architecture"},{name:"Applications",href:"#applications"},{name:"Interactive Demo",href:"#demo"}];function d(){let[e,t]=(0,n.useState)(!1),[a,i]=(0,n.useState)("");return(0,n.useEffect)(()=>{let e=()=>{t(window.scrollY>10);let e=o.map(e=>e.href.substring(1)).find(e=>{let t=document.getElementById(e);if(!t)return!1;let a=t.getBoundingClientRect();return a.top<=100&&a.bottom>=100});e&&i(e)};return window.addEventListener("scroll",e),()=>window.removeEventListener("scroll",e)},[]),(0,s.jsx)("header",{className:"sticky top-0 z-50 transition-all duration-300 ".concat(e?"bg-white/90 dark:bg-slate-900/90 backdrop-blur-sm shadow-sm":"bg-transparent"),children:(0,s.jsx)("div",{className:"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8",children:(0,s.jsxs)("div",{className:"flex justify-between items-center py-4",children:[(0,s.jsx)("div",{className:"flex items-center",children:(0,s.jsx)(l(),{href:"/",className:"flex items-center space-x-2",children:(0,s.jsx)("span",{className:"text-2xl font-bold bg-gradient-to-r from-indigo-600 to-purple-600 dark:from-indigo-400 dark:to-purple-400 text-transparent bg-clip-text",children:"Transformer"})})}),(0,s.jsx)("nav",{className:"hidden md:flex space-x-1",children:o.map(e=>(0,s.jsxs)(l(),{href:e.href,className:"px-3 py-2 rounded-md text-sm font-medium transition-colors relative ".concat(a===e.href.substring(1)?"text-indigo-600 dark:text-indigo-400":"text-gray-700 hover:text-indigo-600 dark:text-gray-300 dark:hover:text-indigo-400"),children:[e.name,a===e.href.substring(1)&&(0,s.jsx)(r.P.div,{layoutId:"navbar-indicator",className:"absolute bottom-0 left-0 right-0 h-0.5 bg-indigo-600 dark:bg-indigo-400",transition:{type:"spring",bounce:.2,duration:.6}})]},e.name))}),(0,s.jsx)("div",{className:"md:hidden",children:(0,s.jsx)("button",{className:"text-gray-700 dark:text-gray-300",children:(0,s.jsx)("svg",{xmlns:"http://www.w3.org/2000/svg",className:"h-6 w-6",fill:"none",viewBox:"0 0 24 24",stroke:"currentColor",children:(0,s.jsx)("path",{strokeLinecap:"round",strokeLinejoin:"round",strokeWidth:2,d:"M4 6h16M4 12h16m-7 6h7"})})})})]})})})}var c=a(2638);function m(e){let{formula:t,block:a=!1,className:r=""}=e,i=(0,n.useRef)(null);return(0,n.useEffect)(()=>{i.current&&c.Ay.render(t,i.current,{throwOnError:!1,displayMode:a})},[t,a]),(0,s.jsx)("div",{ref:i,className:"".concat(a?"formula":"inline-formula"," ").concat(r)})}a(1491);let h=[{name:"Input Embedding",color:"bg-blue-500",description:"Converts input tokens to vector representations"},{name:"Positional Encoding",color:"bg-blue-700",description:"Adds positional information to the embeddings"},{name:"Multi-Head Attention",color:"bg-purple-600",description:"Allows the model to focus on different parts of the input simultaneously"},{name:"Add & Normalize",color:"bg-gray-500",description:"Residual connection and layer normalization"},{name:"Feed Forward Network",color:"bg-green-600",description:"Applies non-linear transformations to the attention outputs"},{name:"Add & Normalize",color:"bg-gray-500",description:"Another residual connection and layer normalization"},{name:"Output Linear & Softmax",color:"bg-red-600",description:"Produces probabilities for output tokens"}];function x(e){let{activeStep:t}=e,[a,i]=(0,n.useState)(0);return(0,n.useEffect)(()=>{i(Math.min(1,(t+1)/h.length))},[t]),(0,s.jsx)("div",{className:"w-full max-w-2xl mx-auto mt-8 mb-12",children:(0,s.jsxs)("div",{className:"relative h-[500px] w-full bg-white dark:bg-slate-900 rounded-lg shadow-md overflow-hidden p-6",children:[(0,s.jsx)("div",{className:"flex flex-col h-full justify-between",children:h.map((e,a)=>(0,s.jsxs)(r.P.div,{className:"p-3 rounded-lg shadow-sm ".concat(e.color," text-white relative mb-2"),initial:{opacity:0,x:-50},animate:{opacity:a<=t?1:.3,x:0,scale:a===t?1.05:1},transition:{duration:.5,delay:.1*a},children:[(0,s.jsx)("h4",{className:"font-medium",children:e.name}),a===t&&(0,s.jsx)(r.P.p,{className:"text-sm mt-1",initial:{opacity:0},animate:{opacity:1},transition:{delay:.3},children:e.description}),a<h.length-1&&(0,s.jsx)(r.P.div,{className:"absolute left-1/2 bottom-0 w-0.5 bg-gray-300 dark:bg-gray-700 -mb-2 transform -translate-x-1/2 z-0 origin-top",style:{height:"20px"},initial:{scaleY:0},animate:{scaleY:+(a<t)},transition:{duration:.3,delay:.1*a+.2}})]},"".concat(e.name,"-").concat(a)))}),(0,s.jsx)(r.P.div,{className:"absolute top-0 left-0 w-full h-full pointer-events-none",style:{opacity:.7},children:t>0&&(0,s.jsx)(r.P.div,{className:"absolute w-3 h-3 rounded-full bg-yellow-400 shadow-md",initial:{top:"10%",left:"40%"},animate:{top:["10%","90%"],left:["40%","60%"],scale:[1,1.2,.9,1]},transition:{duration:3,repeat:1/0,ease:"easeInOut"}})})]})})}let g=["The","transformer","model","revolutionized","natural","language","processing"],u=["Das","Transformer","Modell","revolutionierte","die","Verarbeitung","nat\xfcrlicher","Sprache"];function p(e){let{isActive:t}=e,[a,i]=(0,n.useState)(null),[l,o]=(0,n.useState)(null),[d,c]=(0,n.useState)([]);(0,n.useEffect)(()=>{t&&c(g.map(()=>u.map(()=>Math.random())).map(e=>{let t=e.reduce((e,t)=>e+t,0);return e.map(e=>e/t)}))},[t]);let m=e=>{i(e),setTimeout(()=>{d[e]&&o(d[e].indexOf(Math.max(...d[e])))},300)},h=(e,t)=>{if(!d[e])return"rgba(79, 70, 229, 0)";let a=d[e][t];return"rgba(79, 70, 229, ".concat(a.toFixed(2),")")};return(0,s.jsxs)("div",{className:"w-full max-w-2xl mx-auto mt-8 bg-white dark:bg-slate-800 rounded-lg shadow-md p-6",children:[(0,s.jsx)("h3",{className:"text-lg font-semibold mb-4",children:"Attention Visualization"}),(0,s.jsx)("div",{className:"flex flex-wrap gap-2 mb-8",onMouseLeave:()=>{i(null),o(null)},children:g.map((e,t)=>(0,s.jsx)(r.P.div,{className:"px-3 py-1.5 rounded-md cursor-pointer border transition-all ".concat(a===t?"bg-indigo-100 border-indigo-500 dark:bg-indigo-900 dark:border-indigo-400":"bg-gray-100 border-gray-300 dark:bg-gray-700 dark:border-gray-600"),onMouseEnter:()=>m(t),initial:{opacity:0,y:20},animate:{opacity:1,y:0},transition:{duration:.3,delay:.05*t},children:e},"source-".concat(t)))}),t&&null!==a&&(0,s.jsx)("div",{className:"relative h-20",children:u.map((e,t)=>(0,s.jsx)(r.P.div,{className:"absolute left-0 right-0 h-0.5 transform origin-left",style:{top:"".concat(t/u.length*100,"%"),backgroundColor:h(a,t),opacity:l===t?1:.3},initial:{scaleX:0},animate:{scaleX:1},transition:{duration:.5}},"line-".concat(t)))}),(0,s.jsx)("div",{className:"flex flex-wrap gap-2",children:u.map((e,t)=>(0,s.jsx)(r.P.div,{className:"px-3 py-1.5 rounded-md border transition-all ".concat(l===t?"bg-indigo-100 border-indigo-500 dark:bg-indigo-900 dark:border-indigo-400":"bg-gray-100 border-gray-300 dark:bg-gray-700 dark:border-gray-600"),initial:{opacity:0,y:-20},animate:{opacity:1,y:0},transition:{duration:.3,delay:.05*t+.3},children:e},"target-".concat(t)))}),(0,s.jsx)("p",{className:"text-sm text-gray-600 dark:text-gray-400 mt-6",children:"Hover over the source tokens (English) to see how they attend to the target tokens (German). The intensity of the connection represents the attention weight."})]})}let b=["The","transformer","architecture","uses","self-attention","to","process","sequences"];function f(e){let{isActive:t}=e,[a,i]=(0,n.useState)(null),[l,o]=(0,n.useState)([]);(0,n.useEffect)(()=>{t&&o(b.map(()=>b.map(()=>Math.random())).map(e=>{let t=e.reduce((e,t)=>e+t,0);return e.map(e=>e/t)}))},[t]);let d=e=>{i(e)},c=(e,t)=>null!==a&&l[e]&&e===a?l[e][t]:0;return(0,s.jsxs)("div",{className:"w-full max-w-3xl mx-auto mt-8 bg-white dark:bg-slate-800 rounded-lg shadow-md p-6",children:[(0,s.jsx)("h3",{className:"text-lg font-semibold mb-6",children:"Self-Attention Visualization"}),(0,s.jsxs)("div",{className:"flex flex-col items-center",children:[(0,s.jsx)("p",{className:"text-sm text-gray-600 dark:text-gray-400 mb-4",children:"Hover over a token to see how it attends to other tokens in the sequence."}),(0,s.jsxs)("div",{className:"relative w-full py-10",onMouseLeave:()=>{i(null)},children:[(0,s.jsx)("div",{className:"flex justify-center flex-wrap gap-2 mb-4",children:b.map((e,t)=>(0,s.jsx)(r.P.div,{className:"px-3 py-2 rounded-md cursor-pointer border transition-all ".concat(a===t?"bg-purple-100 border-purple-500 dark:bg-purple-900 dark:border-purple-400":"bg-gray-100 border-gray-300 dark:bg-gray-700 dark:border-gray-600"),onMouseEnter:()=>d(t),initial:{opacity:0,y:20},animate:{opacity:1,y:0},transition:{duration:.3,delay:.05*t},children:e},"token-".concat(t)))}),t&&null!==a&&(0,s.jsx)("div",{className:"absolute top-0 left-0 w-full h-full pointer-events-none",children:b.map((e,t)=>{let n=c(a,t);return(0,s.jsx)(r.P.div,{className:"absolute h-1 rounded-full bg-purple-500",style:{opacity:n,top:"calc(50% - 0.5px)",left:"calc(".concat(a/b.length*50+25,"% - 2rem)"),width:"calc(".concat(Math.abs(t-a)/b.length*100,"% + 4rem)"),transform:t>a?"none":"scaleX(-1)",transformOrigin:t>a?"left":"right"},initial:{scaleX:0},animate:{scaleX:1},transition:{duration:.4}},"attention-".concat(a,"-").concat(t))})})]}),t&&null!==a&&(0,s.jsxs)(r.P.div,{className:"mt-6 overflow-x-auto w-full",initial:{opacity:0},animate:{opacity:1},transition:{duration:.5},children:[(0,s.jsxs)("h4",{className:"text-sm font-medium mb-2",children:['Attention Weights from "',b[a],'"']}),(0,s.jsx)("div",{className:"flex gap-2",children:b.map((e,t)=>{var n;let r=(null===(n=l[a])||void 0===n?void 0:n[t])||0;return(0,s.jsxs)("div",{className:"flex flex-col items-center",children:[(0,s.jsx)("div",{className:"w-10 h-10 rounded-md flex items-center justify-center text-xs",style:{backgroundColor:"rgba(139, 92, 246, ".concat(r.toFixed(2),")"),color:r>.5?"white":"black"},children:r.toFixed(2)}),(0,s.jsx)("div",{className:"text-xs mt-1 truncate max-w-10",children:e})]},"weight-".concat(t))})})]})]})]})}let y=["rgb(239, 68, 68)","rgb(16, 185, 129)","rgb(59, 130, 246)","rgb(139, 92, 246)"];function j(e){let{isActive:t}=e,[a,i]=(0,n.useState)(null),l=e=>{i(a===e?null:e)};return(0,s.jsxs)("div",{className:"w-full max-w-3xl mx-auto mt-8 bg-white dark:bg-slate-800 rounded-lg shadow-md p-6",children:[(0,s.jsx)("h3",{className:"text-lg font-semibold mb-4",children:"Multi-Head Attention"}),(0,s.jsx)("p",{className:"text-sm text-gray-600 dark:text-gray-400 mb-6",children:"Click on an attention head to see what it focuses on in the input sequence. Each head specializes in different aspects of the input."}),(0,s.jsx)("div",{className:"flex justify-center gap-4 mb-8",children:Array.from({length:4}).map((e,t)=>(0,s.jsxs)(r.P.button,{className:"w-16 h-16 rounded-full flex items-center justify-center text-white font-medium transition-transform ".concat(a===t?"ring-4 ring-offset-2 ring-offset-white dark:ring-offset-slate-800":""),style:{backgroundColor:y[t],boxShadow:"0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06)",transform:a===t?"scale(1.1)":"scale(1)"},onClick:()=>l(t),whileHover:{scale:1.05},whileTap:{scale:.95},initial:{opacity:0,y:20},animate:{opacity:1,y:0},transition:{duration:.3,delay:.1*t},children:["Head ",t+1]},"head-".concat(t)))}),(0,s.jsxs)("div",{className:"mb-8",children:[(0,s.jsx)("h4",{className:"text-sm font-medium mb-2",children:"Input Sequence"}),(0,s.jsx)("div",{className:"flex flex-wrap gap-2 p-4 bg-gray-50 dark:bg-slate-700 rounded-md",children:["The","multi-head","attention","allows","the","model","to","focus","on","different","positions","simultaneously"].map((e,t)=>(0,s.jsxs)(r.P.div,{className:"px-3 py-1.5 rounded-md border \n                ".concat(null!==a?"relative":"","\n                bg-white dark:bg-slate-600 border-gray-200 dark:border-gray-500"),initial:{opacity:0},animate:{opacity:1},transition:{duration:.2,delay:.03*t},children:[e,null!==a&&(0,s.jsx)(r.P.div,{className:"absolute inset-0 rounded-md opacity-30",style:{backgroundColor:y[a],opacity:.3*((t+a)%3==0||(t+a)%5==0)},initial:{opacity:0},animate:{opacity:.3*((t+a)%3==0||(t+a)%5==0)},transition:{duration:.3}})]},"token-".concat(t)))})]}),(0,s.jsxs)("div",{children:[(0,s.jsx)("h4",{className:"text-sm font-medium mb-2",children:"What Each Head Focuses On"}),(0,s.jsx)("div",{className:"grid grid-cols-1 md:grid-cols-2 gap-4",children:y.map((e,t)=>(0,s.jsxs)(r.P.div,{className:"p-4 rounded-md border transition-all ".concat(a===t?"border-2 border-current shadow-lg":"border-gray-200 dark:border-gray-700"),style:{color:y[t],opacity:null===a||a===t?1:.5},initial:{opacity:0,scale:.95},animate:{opacity:null===a||a===t?1:.5,scale:1},transition:{duration:.3},children:[(0,s.jsxs)("h5",{className:"font-medium mb-1",children:["Head ",t+1]}),(0,s.jsxs)("p",{className:"text-sm text-gray-600 dark:text-gray-400",children:[0===t&&"Focuses on subject-verb relationships",1===t&&"Attends to adjacent tokens and local context",2===t&&"Focuses on semantic relationships between entities",3===t&&"Looks at long-range dependencies in the sentence"]})]},"focus-".concat(t)))})]}),null!==a&&(0,s.jsxs)(r.P.div,{className:"mt-8 p-4 bg-gray-50 dark:bg-slate-700 rounded-md",initial:{opacity:0,y:20},animate:{opacity:1,y:0},transition:{duration:.4},children:[(0,s.jsx)("h4",{className:"text-sm font-medium mb-2",children:"Concatenated Attention Outputs"}),(0,s.jsx)("div",{className:"flex items-center justify-center",children:Array.from({length:4}).map((e,t)=>(0,s.jsxs)(r.P.div,{className:"h-8 flex items-center justify-center text-white text-xs",style:{width:"".concat(25,"%"),backgroundColor:y[t],opacity:null===a||a===t?1:.5},initial:{scaleY:0},animate:{scaleY:1},transition:{duration:.3,delay:.1*t},children:["Head ",t+1]},"output-".concat(t)))}),(0,s.jsx)("p",{className:"text-xs text-center mt-2 text-gray-600 dark:text-gray-400",children:"The outputs from all heads are concatenated and fed through a linear projection"})]})]})}let N=["The transformer architecture is","Natural language processing has","Deep learning models can","Attention mechanisms allow"],v=e=>({"The transformer architecture is":[" revolutionary for NLP tasks"," based on self-attention mechanisms"," more efficient than RNN-based models"," capable of parallel processing"],"Natural language processing has":[" evolved significantly with transformers"," applications in many industries"," become more accessible to developers"," improved machine translation systems"],"Deep learning models can":[" process vast amounts of data"," learn complex patterns automatically"," transform how we interact with technology"," be fine-tuned for specific tasks"],"Attention mechanisms allow":[" models to focus on relevant information"," for better handling of long sequences"," direct connections between input tokens"," parallel computation of representations"]})[e]||[" revolutionized natural language processing"," changed how we approach AI"," enabled more efficient training"," eliminated the need for recurrence"];function k(e){let{isActive:t}=e,[a,i]=(0,n.useState)(N[0]),[l,o]=(0,n.useState)([]),[d,c]=(0,n.useState)(null),[m,h]=(0,n.useState)(!1),[x,g]=(0,n.useState)(""),u=(0,n.useRef)(null),p=e=>{c(e),g(""),u.current&&clearInterval(u.current);let t=0;u.current=setInterval(()=>{t<=e.length?(g(e.substring(0,t)),t++):u.current&&clearInterval(u.current)},50)};return(0,n.useEffect)(()=>()=>{u.current&&clearInterval(u.current)},[]),(0,s.jsxs)("div",{className:"w-full max-w-3xl mx-auto mt-8 bg-white dark:bg-slate-800 rounded-lg shadow-md p-6",children:[(0,s.jsx)("h3",{className:"text-lg font-semibold mb-4",children:"Interactive Transformer Demo"}),(0,s.jsxs)("div",{className:"space-y-6",children:[(0,s.jsxs)("div",{children:[(0,s.jsx)("label",{htmlFor:"prompt",className:"block text-sm font-medium mb-2",children:"Select a prompt:"}),(0,s.jsx)("select",{id:"prompt",className:"w-full p-2 border rounded-md bg-white dark:bg-slate-700 border-gray-300 dark:border-gray-600",value:a,onChange:e=>{i(e.target.value),o([]),c(null),g("")},children:N.map((e,t)=>(0,s.jsx)("option",{value:e,children:e},t))})]}),(0,s.jsx)("div",{children:(0,s.jsx)("button",{onClick:()=>{h(!0),o([]),c(null),g(""),setTimeout(()=>{o(v(a)),h(!1)},1500)},disabled:m,className:"px-4 py-2 rounded-md text-white font-medium ".concat(m?"bg-gray-400 cursor-not-allowed":"bg-indigo-600 hover:bg-indigo-700 dark:bg-indigo-500 dark:hover:bg-indigo-600"),children:m?(0,s.jsxs)("span",{className:"flex items-center",children:[(0,s.jsxs)("svg",{className:"animate-spin -ml-1 mr-2 h-4 w-4 text-white",xmlns:"http://www.w3.org/2000/svg",fill:"none",viewBox:"0 0 24 24",children:[(0,s.jsx)("circle",{className:"opacity-25",cx:"12",cy:"12",r:"10",stroke:"currentColor",strokeWidth:"4"}),(0,s.jsx)("path",{className:"opacity-75",fill:"currentColor",d:"M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"})]}),"Generating..."]}):"Generate Completions"})}),l.length>0&&(0,s.jsxs)("div",{children:[(0,s.jsx)("h4",{className:"text-sm font-medium mb-2",children:"Select a completion:"}),(0,s.jsx)("div",{className:"space-y-2",children:l.map((e,t)=>(0,s.jsx)(r.P.div,{className:"p-3 border rounded-md cursor-pointer transition-colors ".concat(d===e?"bg-indigo-50 border-indigo-300 dark:bg-indigo-900/30 dark:border-indigo-700":"bg-white dark:bg-slate-700 border-gray-200 dark:border-gray-600 hover:bg-gray-50 dark:hover:bg-slate-600"),onClick:()=>p(e),initial:{opacity:0,y:10},animate:{opacity:1,y:0},transition:{duration:.3,delay:.1*t},children:(0,s.jsx)("span",{className:"font-mono text-sm",children:e})},t))})]}),d&&(0,s.jsxs)(r.P.div,{className:"mt-6",initial:{opacity:0},animate:{opacity:1},transition:{duration:.5},children:[(0,s.jsx)("h4",{className:"text-sm font-medium mb-2",children:"Final output:"}),(0,s.jsx)("div",{className:"p-4 bg-gray-50 dark:bg-slate-700 rounded-md",children:(0,s.jsxs)("p",{className:"font-mono",children:[(0,s.jsx)("span",{className:"text-gray-800 dark:text-gray-200",children:a}),(0,s.jsx)("span",{className:"text-indigo-600 dark:text-indigo-400",children:x}),(0,s.jsx)("span",{className:"animate-pulse",children:"|"})]})})]})]})]})}function w(e){let{isActive:t}=e,[a,i]=(0,n.useState)(0),[l,o]=(0,n.useState)(!1),[d,c]=(0,n.useState)(!1),[m,h]=(0,n.useState)(1e3),x=["The","transformer","architecture","was","created","to","address","limitations","of","RNNs"],g=x.length;(0,n.useEffect)(()=>{if(!t)return;let e=null;return d&&(e=setInterval(()=>{i(e=>e>=g-1?(c(!1),0):e+1)},m)),()=>{e&&clearInterval(e)}},[d,t,g,m]);let u=e=>{i(e),c(!1)},p=(e,t)=>l?Math.exp(-(.5*Math.abs(e-t))):1;return(0,s.jsxs)("div",{className:"w-full max-w-3xl mx-auto mt-8 bg-white dark:bg-slate-800 rounded-lg shadow-md p-6",children:[(0,s.jsx)("h3",{className:"text-lg font-semibold mb-2",children:"RNN Sequential Processing"}),(0,s.jsx)("p",{className:"text-sm text-gray-600 dark:text-gray-400 mb-6",children:"Watch how RNNs process tokens one by one, maintaining a hidden state that gets updated with each token."}),(0,s.jsxs)("div",{className:"flex flex-wrap gap-3 mb-6",children:[(0,s.jsx)("button",{onClick:()=>{c(!d)},className:"px-4 py-2 bg-indigo-600 hover:bg-indigo-700 text-white rounded-md",children:d?"Pause":"Play"}),(0,s.jsx)("button",{onClick:()=>{i(0),c(!1)},className:"px-4 py-2 bg-gray-200 hover:bg-gray-300 dark:bg-gray-700 dark:hover:bg-gray-600 rounded-md",children:"Reset"}),(0,s.jsxs)("div",{className:"flex items-center",children:[(0,s.jsx)("label",{className:"text-sm mr-2",children:"Speed:"}),(0,s.jsxs)("select",{value:m,onChange:e=>h(Number(e.target.value)),className:"px-2 py-1 bg-white dark:bg-slate-700 border rounded-md",children:[(0,s.jsx)("option",{value:2e3,children:"Slow"}),(0,s.jsx)("option",{value:1e3,children:"Normal"}),(0,s.jsx)("option",{value:500,children:"Fast"})]})]}),(0,s.jsx)("div",{className:"flex items-center",children:(0,s.jsxs)("label",{className:"text-sm flex items-center",children:[(0,s.jsx)("input",{type:"checkbox",checked:l,onChange:()=>o(!l),className:"mr-2"}),"Show Vanishing Gradient"]})})]}),(0,s.jsx)("div",{className:"mb-6 p-4 bg-gray-50 dark:bg-slate-700 rounded-md overflow-x-auto",children:(0,s.jsx)("div",{className:"flex min-w-max",children:x.map((e,t)=>(0,s.jsx)(r.P.div,{className:"px-3 py-2 m-1 rounded-md border ".concat(t===a?"bg-indigo-100 border-indigo-500 dark:bg-indigo-900 dark:border-indigo-400":t<a?"bg-gray-100 border-gray-300 dark:bg-gray-700 dark:border-gray-600":"bg-white border-gray-200 dark:bg-slate-800 dark:border-gray-700"),initial:{opacity:0,y:20},animate:{opacity:1,y:0},transition:{duration:.3,delay:.05*t},children:e},"token-".concat(t)))})}),(0,s.jsxs)("div",{className:"relative h-80 bg-gray-50 dark:bg-slate-700 rounded-md p-4 overflow-hidden",children:[(0,s.jsxs)(r.P.div,{className:"absolute left-1/2 transform -translate-x-1/2 top-8 w-40 h-16 rounded-lg bg-green-500 flex items-center justify-center text-white font-medium",animate:{backgroundColor:a>0?"#10B981":"#D1D5DB",x:[null,10,0,-10,0]},transition:{duration:.5,x:{duration:.3,repeat:+(a!==x.length-1),repeatType:"reverse"}},children:["Hidden State",(0,s.jsxs)("span",{className:"absolute -bottom-6 text-xs text-gray-600 dark:text-gray-400",children:["h",(0,s.jsx)("sub",{children:a})]})]}),a<x.length&&(0,s.jsxs)(r.P.div,{className:"absolute left-8 top-40 px-4 py-2 rounded-md bg-indigo-600 text-white",initial:{opacity:0,x:-50},animate:{opacity:1,x:0},children:["Current Token: ",x[a]]},"current-".concat(a)),(0,s.jsx)(r.P.div,{className:"absolute left-1/2 transform -translate-x-1/2 top-40 w-32 h-32 rounded-full border-4 border-purple-500 flex items-center justify-center bg-white dark:bg-slate-800",animate:{borderColor:a<x.length?"#8B5CF6":"#D1D5DB",rotate:a<x.length?[0,5,0,-5,0]:0},transition:{duration:.5,rotate:{duration:.3,repeat:1,repeatType:"reverse"}},children:(0,s.jsxs)("div",{className:"text-center",children:[(0,s.jsx)("div",{className:"font-medium",children:"RNN Cell"}),a<x.length?(0,s.jsx)("div",{className:"text-xs mt-1 text-gray-600 dark:text-gray-400",children:"Processing..."}):(0,s.jsx)("div",{className:"text-xs mt-1 text-green-600 dark:text-green-400",children:"Complete"})]})}),(0,s.jsxs)("svg",{className:"absolute inset-0 w-full h-full pointer-events-none",xmlns:"http://www.w3.org/2000/svg",children:[(0,s.jsx)(r.P.path,{d:"M 200,80 L 200,130",stroke:"#10B981",strokeWidth:"2",fill:"none",markerEnd:"url(#arrowhead)",animate:{opacity:a>0?1:.3}}),(0,s.jsx)(r.P.path,{d:"M 220,130 L 220,80",stroke:"#8B5CF6",strokeWidth:"2",fill:"none",markerEnd:"url(#arrowhead)",animate:{opacity:a>0?1:.3}}),(0,s.jsx)("defs",{children:(0,s.jsx)("marker",{id:"arrowhead",markerWidth:"10",markerHeight:"7",refX:"9",refY:"3.5",orient:"auto",children:(0,s.jsx)("polygon",{points:"0 0, 10 3.5, 0 7"})})})]}),l&&a>0&&(0,s.jsxs)("div",{className:"absolute left-0 bottom-4 w-full",children:[(0,s.jsx)("div",{className:"text-xs text-center mb-2 text-gray-600 dark:text-gray-400",children:"Gradient Strength (backpropagation)"}),(0,s.jsx)("div",{className:"flex justify-center gap-1",children:x.slice(0,a+1).map((e,t)=>{let n=p(a,t);return(0,s.jsx)(r.P.div,{className:"h-4 w-8 rounded-sm",style:{backgroundColor:"rgba(139, 92, 246, ".concat(n,")"),transform:"scaleY(".concat(Math.max(.2,n),")")},initial:{opacity:0},animate:{opacity:1},transition:{duration:.5}},"gradient-".concat(t))})}),(0,s.jsx)("div",{className:"flex justify-center gap-1 mt-1",children:x.slice(0,a+1).map((e,t)=>(0,s.jsx)("div",{className:"text-xs w-8 text-center truncate",children:e},"token-grad-".concat(t)))})]})]}),(0,s.jsxs)("div",{className:"mt-6 flex flex-col items-center",children:[(0,s.jsxs)("div",{className:"text-sm mb-2",children:["Step: ",a," / ",g]}),(0,s.jsx)("div",{className:"w-full max-w-md bg-gray-200 dark:bg-gray-700 rounded-full h-2.5 mb-4",children:(0,s.jsx)("div",{className:"bg-indigo-600 h-2.5 rounded-full",style:{width:"".concat(a/g*100,"%")}})}),(0,s.jsx)("div",{className:"flex gap-2 flex-wrap justify-center",children:Array.from({length:g+1}).map((e,t)=>(0,s.jsx)("button",{onClick:()=>u(t),className:"w-8 h-8 rounded-full ".concat(a===t?"bg-indigo-600 text-white":"bg-gray-200 dark:bg-gray-700 hover:bg-gray-300 dark:hover:bg-gray-600"),children:t},t))})]})]})}let T=["The","quick","brown","fox","jumps","over","the","lazy","dog","while","the","cat","watches","from","a","comfortable","distance"],S=["Der","schnelle","braune","Fuchs","springt","\xfcber","den","faulen","Hund","w\xe4hrend","die","Katze","aus","einer","bequemen","Entfernung","zuschaut"];function q(e){let{isActive:t}=e,[a,i]=(0,n.useState)(0),[l,o]=(0,n.useState)(!1),[d,c]=(0,n.useState)(!1),m=l?T:T.slice(0,6),h=l?S:S.slice(0,6);return(0,n.useEffect)(()=>{t&&i(0)},[t]),(0,s.jsxs)("div",{className:"w-full max-w-3xl mx-auto mt-8 bg-white dark:bg-slate-800 rounded-lg shadow-md p-6",children:[(0,s.jsx)("h3",{className:"text-lg font-semibold mb-2",children:"The Information Bottleneck Problem"}),(0,s.jsx)("p",{className:"text-sm text-gray-600 dark:text-gray-400 mb-6",children:"See how traditional encoder-decoder models compress all input information into a fixed-size context vector, creating a bottleneck that loses information, especially with longer sequences."}),(0,s.jsxs)("div",{className:"flex flex-wrap gap-3 mb-6",children:[(0,s.jsx)("button",{onClick:()=>{o(e=>!e),i(0)},className:"px-4 py-2 bg-indigo-600 hover:bg-indigo-700 text-white rounded-md",children:l?"Use Short Sequence":"Use Long Sequence"}),(0,s.jsxs)("label",{className:"inline-flex items-center",children:[(0,s.jsx)("input",{type:"checkbox",checked:d,onChange:()=>c(!d),className:"mr-2"}),(0,s.jsx)("span",{className:"text-sm",children:"Show Information Loss"})]})]}),(0,s.jsxs)("div",{className:"relative h-96 bg-gray-50 dark:bg-slate-700 rounded-md p-4 overflow-hidden",children:[(0,s.jsxs)("div",{className:"absolute top-4 left-4 w-1/2 pr-4",children:[(0,s.jsx)("div",{className:"text-sm font-medium mb-2",children:"Source Sentence (Input)"}),(0,s.jsx)("div",{className:"flex flex-wrap gap-1",children:m.map((e,t)=>(0,s.jsx)(r.P.div,{className:"px-2 py-1 text-sm rounded-md ".concat(a>=1?"bg-blue-100 dark:bg-blue-900/30 border border-blue-300 dark:border-blue-800":"bg-white dark:bg-slate-800 border border-gray-300 dark:border-gray-700"),initial:{opacity:0},animate:{opacity:1,y:a>=1&&a<3?[-5,0]:0},transition:{duration:.3,delay:a>=1?.05*t:0,y:{repeat:1===a?1/0:0,repeatType:"reverse",duration:.5}},children:e},"source-".concat(t)))})]}),(0,s.jsxs)("div",{className:"absolute top-4 right-4 w-1/2 pl-4 text-right",children:[(0,s.jsx)("div",{className:"text-sm font-medium mb-2",children:"Target Sentence (Output)"}),(0,s.jsx)("div",{className:"flex flex-wrap gap-1 justify-end",children:h.map((e,t)=>(0,s.jsx)(r.P.div,{className:"px-2 py-1 text-sm rounded-md ".concat(a>=3?"bg-purple-100 dark:bg-purple-900/30 border border-purple-300 dark:border-purple-800":"bg-white dark:bg-slate-800 border border-gray-300 dark:border-gray-700"),initial:{opacity:0},animate:{opacity:a>=3?1:.3,y:a>=3?[5,0]:0},transition:{duration:.3,delay:a>=3?.1*t:0},children:e},"target-".concat(t)))})]}),(0,s.jsx)(r.P.div,{className:"absolute top-1/3 left-1/4 transform -translate-x-1/2 -translate-y-1/2 w-32 h-32 bg-blue-500 dark:bg-blue-700 rounded-lg flex items-center justify-center text-white font-medium",animate:{scale:a>=1?[1,1.05,1]:1,backgroundColor:a>=1?"#3B82F6":"#93C5FD"},transition:{duration:.5,scale:{repeat:1===a?1/0:0,repeatType:"reverse",duration:1}},children:"Encoder"}),(0,s.jsx)(r.P.div,{className:"absolute top-1/3 right-1/4 transform translate-x-1/2 -translate-y-1/2 w-32 h-32 bg-purple-500 dark:bg-purple-700 rounded-lg flex items-center justify-center text-white font-medium",animate:{scale:a>=3?[1,1.05,1]:1,backgroundColor:a>=3?"#8B5CF6":"#C4B5FD"},transition:{duration:.5,scale:{repeat:3===a?1/0:0,repeatType:"reverse",duration:1}},children:"Decoder"}),(0,s.jsxs)(r.P.div,{className:"absolute top-1/3 left-1/2 transform -translate-x-1/2 -translate-y-1/2 w-20 h-20 bg-green-500 dark:bg-green-700 rounded-full flex items-center justify-center text-white text-sm text-center",initial:{scale:0},animate:{scale:+(a>=2),backgroundColor:a>=2?d&&l?"#EF4444":"#10B981":"#D1FAE5"},transition:{duration:.5},children:["Context Vector",d&&l&&a>=2&&(0,s.jsx)(r.P.div,{className:"absolute -bottom-6 text-xs text-red-600 dark:text-red-400 whitespace-nowrap",initial:{opacity:0},animate:{opacity:1},transition:{delay:.5},children:"Information Lost!"})]}),(0,s.jsxs)("svg",{className:"absolute inset-0 w-full h-full pointer-events-none",xmlns:"http://www.w3.org/2000/svg",children:[(0,s.jsx)(r.P.path,{d:"M 110,120 L 190,120",stroke:a>=2?"#10B981":"#D1D5DB",strokeWidth:"3",strokeDasharray:a>=2?"0":"5,5",fill:"none",markerEnd:"url(#arrowhead)",animate:{opacity:a>=1?1:.3,stroke:a>=2?d&&l?"#EF4444":"#10B981":"#D1D5DB"},transition:{duration:.5}}),(0,s.jsx)(r.P.path,{d:"M 210,120 L 290,120",stroke:a>=3?"#8B5CF6":"#D1D5DB",strokeWidth:"3",strokeDasharray:a>=3?"0":"5,5",fill:"none",markerEnd:"url(#arrowhead)",animate:{opacity:a>=2?1:.3,stroke:a>=3?d&&l?"#EF4444":"#8B5CF6":"#D1D5DB"},transition:{duration:.5}}),d&&l&&a>=2&&(0,s.jsx)(r.P.g,{initial:{opacity:0},animate:{opacity:1},transition:{delay:.3},children:m.slice(6).map((e,t)=>(0,s.jsx)(r.P.circle,{cx:125+t%5*10,cy:100-10*Math.floor(t/5),r:"3",fill:"#EF4444",initial:{opacity:0},animate:{opacity:1,y:[0,30,60,100]},transition:{delay:.5+.1*t,y:{duration:1.5,ease:"easeIn"},opacity:{duration:1.5,times:[0,.8,1],values:[1,1,0]}}},"info-loss-".concat(t)))}),(0,s.jsx)("defs",{children:(0,s.jsx)("marker",{id:"arrowhead",markerWidth:"10",markerHeight:"7",refX:"9",refY:"3.5",orient:"auto",children:(0,s.jsx)("polygon",{points:"0 0, 10 3.5, 0 7",fill:a>=3?"#8B5CF6":"#D1D5DB"})})})]}),a>=3&&(0,s.jsxs)(r.P.div,{className:"absolute bottom-8 right-1/4 transform translate-x-1/2 w-40 text-center",initial:{opacity:0,y:20},animate:{opacity:1,y:0},transition:{duration:.5},children:[(0,s.jsx)("div",{className:"text-sm font-medium mb-2",children:"Decoder Output"}),(0,s.jsx)("div",{className:"text-xs text-gray-600 dark:text-gray-400",children:d&&l?(0,s.jsx)(s.Fragment,{children:"Generating translation with degraded quality due to information loss"}):(0,s.jsx)(s.Fragment,{children:"Generating translation token by token"})})]}),l&&d&&a>=2&&(0,s.jsxs)(r.P.div,{className:"absolute bottom-4 left-1/2 transform -translate-x-1/2 max-w-xs text-center",initial:{opacity:0,y:20},animate:{opacity:1,y:0},transition:{duration:.5,delay:.5},children:[(0,s.jsx)("div",{className:"text-sm font-medium text-red-600 dark:text-red-400 mb-1",children:"The Bottleneck Problem"}),(0,s.jsx)("div",{className:"text-xs text-gray-600 dark:text-gray-400",children:"A fixed-size context vector struggles to retain all information from longer sequences, resulting in degraded translation quality."})]})]}),(0,s.jsxs)("div",{className:"mt-6 flex justify-between items-center",children:[(0,s.jsx)("button",{onClick:()=>{i(0)},className:"px-4 py-2 bg-gray-200 hover:bg-gray-300 dark:bg-gray-700 dark:hover:bg-gray-600 rounded-md",disabled:0===a,children:"Reset"}),(0,s.jsxs)("div",{className:"text-sm",children:["Stage: ",a," / 3"]}),(0,s.jsx)("button",{onClick:()=>{i(e=>Math.min(e+1,3))},className:"px-4 py-2 bg-indigo-600 hover:bg-indigo-700 text-white rounded-md",disabled:a>=3,children:"Next Stage"})]})]})}function P(e){let{isActive:t}=e,[a,i]=(0,n.useState)("rnn"),l={rnn:{name:"Vanilla RNN",description:"Basic recurrent neural network that processes sequences one token at a time, maintaining a hidden state.",formula:"h_t = \\tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h)",advantages:["Simple architecture","Efficient for short sequences","Minimal parameter count"],limitations:["Vanishing/exploding gradients","Difficulty capturing long-range dependencies","Sequential processing (slow)","Fixed-size hidden state"],year:1986,complexity:3,parallelization:1,longRangeDependency:2},lstm:{name:"LSTM (Long Short-Term Memory)",description:"Enhanced RNN with gating mechanisms to better control information flow and address vanishing gradients.",formula:"\\begin{aligned} f_t &= \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f) \\\\ i_t &= \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i) \\\\ \\tilde{C}_t &= \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C) \\\\ C_t &= f_t * C_{t-1} + i_t * \\tilde{C}_t \\\\ o_t &= \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o) \\\\ h_t &= o_t * \\tanh(C_t) \\end{aligned}",advantages:["Better at capturing long dependencies","Addresses vanishing gradient problem","Separate memory cell (C_t)","Control gates for information flow"],limitations:["Still sequential processing","Complex architecture","Higher computational cost","Still limited context window"],year:1997,complexity:7,parallelization:1,longRangeDependency:6},gru:{name:"GRU (Gated Recurrent Unit)",description:"Simplified version of LSTM with fewer parameters but similar performance on many tasks.",formula:"\\begin{aligned} z_t &= \\sigma(W_z \\cdot [h_{t-1}, x_t] + b_z) \\\\ r_t &= \\sigma(W_r \\cdot [h_{t-1}, x_t] + b_r) \\\\ \\tilde{h}_t &= \\tanh(W \\cdot [r_t * h_{t-1}, x_t] + b) \\\\ h_t &= (1 - z_t) * h_{t-1} + z_t * \\tilde{h}_t \\end{aligned}",advantages:["Simpler than LSTM (fewer parameters)","Good long-term dependency capture","Update and reset gates","Often comparable performance to LSTM"],limitations:["Still sequential processing","Limited parallelization","Fixed computational path","Still has context limitations"],year:2014,complexity:6,parallelization:1,longRangeDependency:5},transformer:{name:"Transformer",description:"Attention-based architecture that processes all tokens in parallel and directly models relationships between any positions.",formula:"\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V",advantages:["Full parallelization during training","Direct connections between any positions","Multi-head attention for different relationships","No information bottleneck","State-of-the-art performance"],limitations:["Quadratic complexity with sequence length","Higher memory requirements","Lacks built-in sequence order understanding","Requires positional encodings"],year:2017,complexity:8,parallelization:9,longRangeDependency:9}},o=e=>{i(e)},d=l[a];return(0,s.jsxs)("div",{className:"w-full max-w-4xl mx-auto mt-8 bg-white dark:bg-slate-800 rounded-lg shadow-md p-6",children:[(0,s.jsx)("h3",{className:"text-lg font-semibold mb-4",children:"Technical Comparison: RNN Variants vs. Transformers"}),(0,s.jsx)("div",{className:"flex flex-wrap gap-2 mb-6 border-b border-gray-200 dark:border-gray-700",children:Object.keys(l).map(e=>(0,s.jsx)("button",{onClick:()=>o(e),className:"px-4 py-2 text-sm font-medium rounded-t-lg transition-colors ".concat(a===e?"bg-indigo-600 text-white":"bg-gray-100 dark:bg-gray-700 hover:bg-gray-200 dark:hover:bg-gray-600"),children:l[e].name},e))}),(0,s.jsxs)("div",{className:"grid grid-cols-1 lg:grid-cols-2 gap-6",children:[(0,s.jsx)("div",{children:(0,s.jsxs)(r.P.div,{initial:{opacity:0,x:-20},animate:{opacity:1,x:0},transition:{duration:.3},className:"space-y-4",children:[(0,s.jsxs)("div",{children:[(0,s.jsx)("h4",{className:"text-xl font-semibold mb-1",children:d.name}),(0,s.jsxs)("div",{className:"text-sm text-gray-500 dark:text-gray-400",children:["Introduced around ",d.year]}),(0,s.jsx)("p",{className:"mt-2 text-gray-700 dark:text-gray-300",children:d.description})]}),(0,s.jsxs)("div",{children:[(0,s.jsx)("h5",{className:"font-medium mb-2",children:"Key Equation"}),(0,s.jsx)("div",{className:"bg-gray-50 dark:bg-slate-700 p-4 rounded-md overflow-x-auto",children:(0,s.jsx)(m,{formula:d.formula,block:!0})})]}),(0,s.jsxs)("div",{className:"grid grid-cols-1 sm:grid-cols-2 gap-4",children:[(0,s.jsxs)("div",{children:[(0,s.jsx)("h5",{className:"font-medium mb-2 text-green-600 dark:text-green-400",children:"Advantages"}),(0,s.jsx)("ul",{className:"list-disc pl-5 space-y-1 text-sm",children:d.advantages.map((e,t)=>(0,s.jsx)("li",{children:e},t))})]}),(0,s.jsxs)("div",{children:[(0,s.jsx)("h5",{className:"font-medium mb-2 text-red-600 dark:text-red-400",children:"Limitations"}),(0,s.jsx)("ul",{className:"list-disc pl-5 space-y-1 text-sm",children:d.limitations.map((e,t)=>(0,s.jsx)("li",{children:e},t))})]})]})]},a)}),(0,s.jsxs)("div",{className:"bg-gray-50 dark:bg-slate-700 p-4 rounded-lg",children:[(0,s.jsx)("h4",{className:"text-lg font-medium mb-4",children:"Performance Comparison"}),(0,s.jsxs)("div",{className:"mb-6",children:[(0,s.jsxs)("div",{className:"flex justify-between mb-1",children:[(0,s.jsx)("span",{className:"text-sm font-medium",children:"Computational Complexity"}),(0,s.jsxs)("span",{className:"text-sm font-medium",children:[d.complexity,"/10"]})]}),(0,s.jsx)("div",{className:"w-full bg-gray-200 dark:bg-gray-600 rounded-full h-2.5",children:(0,s.jsx)(r.P.div,{className:"bg-blue-600 h-2.5 rounded-full",initial:{width:0},animate:{width:"".concat(d.complexity/10*100,"%")},transition:{duration:.5}})}),(0,s.jsx)("div",{className:"text-xs text-gray-500 dark:text-gray-400 mt-1",children:"transformer"===a?"Higher complexity but with significant performance benefits":"Lower complexity but also lower performance ceiling"})]}),(0,s.jsxs)("div",{className:"mb-6",children:[(0,s.jsxs)("div",{className:"flex justify-between mb-1",children:[(0,s.jsx)("span",{className:"text-sm font-medium",children:"Parallelization Capability"}),(0,s.jsxs)("span",{className:"text-sm font-medium",children:[d.parallelization,"/10"]})]}),(0,s.jsx)("div",{className:"w-full bg-gray-200 dark:bg-gray-600 rounded-full h-2.5",children:(0,s.jsx)(r.P.div,{className:"bg-purple-600 h-2.5 rounded-full",initial:{width:0},animate:{width:"".concat(d.parallelization/10*100,"%")},transition:{duration:.5}})}),(0,s.jsx)("div",{className:"text-xs text-gray-500 dark:text-gray-400 mt-1",children:"transformer"===a?"All tokens processed in parallel during training, significant speed advantage":"Sequential processing limits parallelization and training speed"})]}),(0,s.jsxs)("div",{className:"mb-6",children:[(0,s.jsxs)("div",{className:"flex justify-between mb-1",children:[(0,s.jsx)("span",{className:"text-sm font-medium",children:"Long-Range Dependency Modeling"}),(0,s.jsxs)("span",{className:"text-sm font-medium",children:[d.longRangeDependency,"/10"]})]}),(0,s.jsx)("div",{className:"w-full bg-gray-200 dark:bg-gray-600 rounded-full h-2.5",children:(0,s.jsx)(r.P.div,{className:"bg-green-600 h-2.5 rounded-full",initial:{width:0},animate:{width:"".concat(d.longRangeDependency/10*100,"%")},transition:{duration:.5}})}),(0,s.jsx)("div",{className:"text-xs text-gray-500 dark:text-gray-400 mt-1",children:"transformer"===a?"Direct attention connections enable excellent long-range dependency modeling":"Information must flow through many timesteps, weakening long-range connections"})]}),(0,s.jsxs)("div",{className:"mt-8",children:[(0,s.jsx)("h4",{className:"text-sm font-medium mb-2",children:"Timeline of Development"}),(0,s.jsx)("div",{className:"relative h-12 bg-gray-200 dark:bg-gray-600 rounded-md",children:Object.keys(l).map(e=>{let t=(l[e].year-1985)/35*100;return(0,s.jsxs)(r.P.div,{className:"absolute bottom-0 transform -translate-x-1/2 ".concat(a===e?"z-10":"z-0"),style:{left:"".concat(t,"%")},initial:{y:20},animate:{y:10*(a!==e)},transition:{duration:.3},children:[(0,s.jsx)("div",{className:"h-4 w-4 rounded-full ".concat(a===e?"bg-indigo-600":"bg-gray-400 dark:bg-gray-500")}),(0,s.jsx)("div",{className:"text-xs font-medium mt-1 ".concat(a===e?"text-indigo-600 dark:text-indigo-400":"text-gray-600 dark:text-gray-400"),children:l[e].year}),(0,s.jsx)("div",{className:"text-[10px] whitespace-nowrap ".concat(a===e?"text-indigo-600 dark:text-indigo-400 font-medium":"text-gray-500 dark:text-gray-400"),children:l[e].name})]},e)})})]})]})]}),"transformer"===a&&(0,s.jsxs)(r.P.div,{className:"mt-6 p-4 border-l-4 border-indigo-500 bg-indigo-50 dark:bg-indigo-900/20 dark:border-indigo-700",initial:{opacity:0,y:20},animate:{opacity:1,y:0},transition:{duration:.5},children:[(0,s.jsx)("h4",{className:"font-medium mb-2",children:"Key Innovation of Transformers"}),(0,s.jsxs)("p",{className:"text-sm text-gray-700 dark:text-gray-300",children:["The fundamental breakthrough of the Transformer architecture is replacing sequential processing with",(0,s.jsx)("strong",{children:" self-attention"}),", allowing direct modeling of relationships between any positions in the sequence. This eliminates the need for information to flow through intermediate states, enabling better parallelization and more effective modeling of long-range dependencies."]})]})]})}function _(){let[e,t]=(0,n.useState)("introduction"),[a,i]=(0,n.useState)(0),l={introduction:(0,n.useRef)(null),seqModels:(0,n.useRef)(null),attention:(0,n.useRef)(null),architecture:(0,n.useRef)(null),applications:(0,n.useRef)(null),demo:(0,n.useRef)(null)};(0,n.useEffect)(()=>{let e=()=>{let e=window.scrollY+100;Object.entries(l).forEach(a=>{let[s,n]=a;if(!n.current)return;let{offsetTop:r,offsetHeight:i}=n.current;e>=r&&e<r+i&&t(s)})};return window.addEventListener("scroll",e),()=>window.removeEventListener("scroll",e)},[]);let o=e=>{let t=l[e].current;t&&window.scrollTo({top:t.offsetTop-80,behavior:"smooth"})};return(0,s.jsxs)("div",{className:"min-h-screen",children:[(0,s.jsx)(d,{}),(0,s.jsx)("section",{ref:l.introduction,id:"introduction",className:"section-container pt-24 sm:pt-32",children:(0,s.jsxs)("div",{className:"flex flex-col lg:flex-row items-center justify-between gap-12",children:[(0,s.jsxs)("div",{className:"lg:w-1/2",children:[(0,s.jsxs)(r.P.h1,{className:"text-4xl sm:text-5xl font-bold mb-6",initial:{opacity:0,y:20},animate:{opacity:1,y:0},transition:{duration:.5},children:["Understanding the ",(0,s.jsx)("span",{className:"highlight",children:"Transformer"})," Architecture"]}),(0,s.jsx)(r.P.p,{className:"text-lg mb-8 text-gray-700 dark:text-gray-300",initial:{opacity:0,y:20},animate:{opacity:1,y:0},transition:{duration:.5,delay:.1},children:"A step-by-step visual journey through the architecture that revolutionized natural language processing and beyond."}),(0,s.jsxs)(r.P.div,{className:"flex gap-4 mb-8",initial:{opacity:0,y:20},animate:{opacity:1,y:0},transition:{duration:.5,delay:.2},children:[(0,s.jsx)("button",{onClick:()=>o("seqModels"),className:"px-6 py-3 bg-indigo-600 hover:bg-indigo-700 text-white font-medium rounded-md transition-colors",children:"Begin Learning"}),(0,s.jsx)("button",{onClick:()=>o("demo"),className:"px-6 py-3 border border-gray-300 dark:border-gray-700 hover:bg-gray-50 dark:hover:bg-gray-800 rounded-md transition-colors",children:"Jump to Demo"})]})]}),(0,s.jsx)(r.P.div,{className:"lg:w-1/2",initial:{opacity:0,scale:.9},animate:{opacity:1,scale:1},transition:{duration:.5,delay:.3},children:(0,s.jsx)("div",{className:"aspect-video relative bg-gradient-to-br from-indigo-500 to-purple-600 rounded-lg shadow-xl overflow-hidden",children:(0,s.jsx)("div",{className:"absolute inset-0 flex items-center justify-center text-white text-lg font-medium",children:"Transformers: Attention is All You Need"})})})]})}),(0,s.jsx)("section",{ref:l.seqModels,id:"seq-models",className:"section-container",children:(0,s.jsxs)("div",{className:"max-w-4xl mx-auto",children:[(0,s.jsx)("h2",{className:"text-3xl font-bold mb-6",children:"Sequence-to-Sequence Modeling"}),(0,s.jsx)("p",{className:"mb-6 text-gray-700 dark:text-gray-300",children:"Sequence-to-sequence models are a class of neural networks designed to transform one sequence into another. Before the Transformer architecture, these tasks primarily relied on Recurrent Neural Networks (RNNs) and their variants. Let's explore how these models work and the fundamental limitations that motivated the development of Transformers."}),(0,s.jsxs)("div",{className:"card mb-8",children:[(0,s.jsx)("h3",{className:"text-xl font-semibold mb-4",children:"What is Sequence-to-Sequence Learning?"}),(0,s.jsx)("p",{className:"mb-4 text-gray-700 dark:text-gray-300",children:"Sequence-to-sequence (Seq2Seq) learning refers to the task of converting an input sequence into a corresponding output sequence, where the lengths of input and output may differ. Applications include:"}),(0,s.jsxs)("div",{className:"grid grid-cols-1 md:grid-cols-3 gap-4 mb-6",children:[(0,s.jsxs)("div",{className:"p-3 bg-blue-50 dark:bg-blue-900/20 rounded-md",children:[(0,s.jsx)("h4",{className:"font-medium mb-2 text-blue-700 dark:text-blue-400",children:"Machine Translation"}),(0,s.jsx)("p",{className:"text-sm",children:"Converting text from one language to another, where grammar and word order may differ significantly."})]}),(0,s.jsxs)("div",{className:"p-3 bg-purple-50 dark:bg-purple-900/20 rounded-md",children:[(0,s.jsx)("h4",{className:"font-medium mb-2 text-purple-700 dark:text-purple-400",children:"Text Summarization"}),(0,s.jsx)("p",{className:"text-sm",children:"Generating a concise summary from a longer document while preserving key information."})]}),(0,s.jsxs)("div",{className:"p-3 bg-green-50 dark:bg-green-900/20 rounded-md",children:[(0,s.jsx)("h4",{className:"font-medium mb-2 text-green-700 dark:text-green-400",children:"Speech Recognition"}),(0,s.jsx)("p",{className:"text-sm",children:"Converting audio waveforms into text transcriptions."})]})]}),(0,s.jsx)("p",{className:"text-gray-700 dark:text-gray-300",children:"The core challenge in sequence-to-sequence learning is effectively modeling complex dependencies between input and output elements, regardless of their positions in the sequences. This becomes particularly difficult when dealing with long-range dependencies in natural language."})]}),(0,s.jsxs)("div",{className:"mb-12",children:[(0,s.jsx)("h3",{className:"text-xl font-semibold mb-4",children:"Recurrent Neural Networks (RNNs)"}),(0,s.jsx)("p",{className:"mb-6 text-gray-700 dark:text-gray-300",children:'RNNs were the first major architecture for sequence modeling. They process text one token at a time, maintaining an internal "memory" (hidden state) that gets updated at each step. This sequential nature creates fundamental limitations.'}),(0,s.jsx)(w,{isActive:"seqModels"===e})]}),(0,s.jsxs)("div",{className:"mb-12",children:[(0,s.jsx)("h3",{className:"text-xl font-semibold mb-4",children:"Technical Evolution of Sequence Models"}),(0,s.jsx)("p",{className:"mb-6 text-gray-700 dark:text-gray-300",children:"As researchers attempted to overcome the limitations of vanilla RNNs, several enhanced architectures were developed. Let's compare the technical details of these models and see how they eventually led to the Transformer breakthrough."}),(0,s.jsx)(P,{isActive:"seqModels"===e})]}),(0,s.jsxs)("div",{className:"card mb-8",children:[(0,s.jsx)("h3",{className:"text-xl font-semibold mb-4",children:"Limitations of RNN-based Models"}),(0,s.jsxs)("ul",{className:"list-disc pl-6 space-y-3 text-gray-700 dark:text-gray-300",children:[(0,s.jsxs)("li",{children:[(0,s.jsx)("strong",{children:"Sequential Processing:"})," RNNs process tokens one by one, making them unable to parallelize and slow for long sequences. This sequential nature means that training time scales linearly with sequence length, becoming impractical for very long sequences."]}),(0,s.jsxs)("li",{children:[(0,s.jsx)("strong",{children:"Long-range Dependencies:"}),' Information from early tokens tends to get diluted as sequences get longer. For example, in the sentence "The cat, which was sitting on the mat that was purchased last week from the store downtown, is brown," the connection between "cat" and "brown" becomes difficult for RNNs to maintain.']}),(0,s.jsxs)("li",{children:[(0,s.jsx)("strong",{children:"Vanishing Gradients:"})," Gradients can vanish during backpropagation through time, making it difficult to learn long-range dependencies. As the error signal propagates backward through many time steps, it tends to diminish exponentially, effectively preventing learning from distant contexts."]}),(0,s.jsxs)("li",{children:[(0,s.jsx)("strong",{children:"Lack of Parallel Processing:"})," RNNs process tokens sequentially, making them inefficient on modern GPU hardware which excels at parallel computation. This makes training on large datasets prohibitively time-consuming."]})]})]}),(0,s.jsxs)("div",{className:"mb-12",children:[(0,s.jsx)("h3",{className:"text-xl font-semibold mb-4",children:"The Encoder-Decoder Framework"}),(0,s.jsx)("p",{className:"mb-4 text-gray-700 dark:text-gray-300",children:"Traditional sequence-to-sequence models used an encoder-decoder architecture. The encoder processes the entire input sequence and compresses it into a context vector. The decoder then generates the output sequence based on this context vector."}),(0,s.jsx)("div",{className:"bg-white dark:bg-slate-800 p-6 rounded-lg shadow-md",children:(0,s.jsxs)("div",{className:"flex flex-col md:flex-row gap-4 justify-center items-center",children:[(0,s.jsxs)("div",{className:"p-4 border border-indigo-200 dark:border-indigo-800 rounded-md bg-indigo-50 dark:bg-indigo-900/30 w-full md:w-2/5",children:[(0,s.jsx)("h4",{className:"font-medium text-center mb-2",children:"Encoder"}),(0,s.jsx)("p",{className:"text-sm text-center text-gray-600 dark:text-gray-400",children:"Processes the input sequence and compresses it into a context vector"})]}),(0,s.jsx)("div",{className:"text-center text-gray-400",children:"→"}),(0,s.jsx)("div",{className:"w-full md:w-1/5 py-2 text-center",children:(0,s.jsx)("div",{className:"bg-gray-200 dark:bg-gray-700 p-2 rounded-md mx-auto w-full max-w-[100px]",children:(0,s.jsx)("p",{className:"text-xs",children:"Context Vector"})})}),(0,s.jsx)("div",{className:"text-center text-gray-400",children:"→"}),(0,s.jsxs)("div",{className:"p-4 border border-purple-200 dark:border-purple-800 rounded-md bg-purple-50 dark:bg-purple-900/30 w-full md:w-2/5",children:[(0,s.jsx)("h4",{className:"font-medium text-center mb-2",children:"Decoder"}),(0,s.jsx)("p",{className:"text-sm text-center text-gray-600 dark:text-gray-400",children:"Generates the output sequence token by token based on the context"})]})]})})]}),(0,s.jsxs)("div",{className:"mb-12",children:[(0,s.jsx)("h3",{className:"text-xl font-semibold mb-4",children:"The Information Bottleneck Problem"}),(0,s.jsx)("p",{className:"mb-6 text-gray-700 dark:text-gray-300",children:"A critical limitation of the encoder-decoder framework was the bottleneck created by compressing all information into a fixed-size context vector. This bottleneck became particularly problematic for long sequences where important information could be lost."}),(0,s.jsx)(q,{isActive:"seqModels"===e})]}),(0,s.jsxs)("div",{className:"card p-5 border-l-4 border-indigo-500",children:[(0,s.jsx)("h3",{className:"text-xl font-semibold mb-2",children:"Motivating the Transformer Architecture"}),(0,s.jsx)("p",{className:"text-gray-700 dark:text-gray-300 mb-4",children:"The key problems that motivated the development of the Transformer architecture can be summarized as:"}),(0,s.jsxs)("ol",{className:"list-decimal pl-6 space-y-2 text-gray-700 dark:text-gray-300",children:[(0,s.jsxs)("li",{children:[(0,s.jsx)("strong",{children:"Sequential bottlenecks"})," in RNNs that prevented parallelization"]}),(0,s.jsxs)("li",{children:[(0,s.jsx)("strong",{children:"Vanishing signal"})," across long sequences that made learning long-range dependencies difficult"]}),(0,s.jsxs)("li",{children:[(0,s.jsx)("strong",{children:"Information compression"})," into a fixed-size context vector that created an information bottleneck"]}),(0,s.jsxs)("li",{children:[(0,s.jsx)("strong",{children:"High computational cost"})," of training RNN models, especially for long sequences"]})]}),(0,s.jsx)("p",{className:"mt-4 text-gray-700 dark:text-gray-300",children:'The Transformer architecture, introduced in the paper "Attention is All You Need" (Vaswani et al., 2017), addressed all these limitations by replacing recurrence entirely with attention mechanisms. The key insight was that direct connections between all positions in a sequence could eliminate sequential processing while better capturing long-range dependencies.'})]})]})}),(0,s.jsx)("section",{ref:l.attention,id:"attention",className:"section-container bg-gray-50 dark:bg-slate-900",children:(0,s.jsxs)("div",{className:"max-w-4xl mx-auto",children:[(0,s.jsx)("h2",{className:"text-3xl font-bold mb-6",children:"The Attention Mechanism"}),(0,s.jsx)("p",{className:"mb-6 text-gray-700 dark:text-gray-300",children:'Attention mechanisms were introduced to address the bottleneck problem by allowing the decoder to "look back" at the source sequence. Instead of relying solely on a fixed context vector, the decoder could focus on relevant parts of the input for each output token.'}),(0,s.jsxs)("div",{className:"card mb-8",children:[(0,s.jsx)("h3",{className:"text-xl font-semibold mb-4",children:"How Attention Works"}),(0,s.jsx)("p",{className:"mb-4 text-gray-700 dark:text-gray-300",children:"The attention mechanism computes a weighted sum of all encoder hidden states, with weights determined by their relevance to the current decoder state."}),(0,s.jsxs)("div",{className:"formula mb-6",children:[(0,s.jsx)("p",{className:"text-center mb-2",children:"The attention score between query q and key k:"}),(0,s.jsx)(m,{formula:"score(q, k) = \\frac{q \\cdot k}{\\sqrt{d_k}}",block:!0}),(0,s.jsx)("p",{className:"text-center mb-2 mt-4",children:"The attention weights after softmax:"}),(0,s.jsx)(m,{formula:"attention(Q, K, V) = softmax\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V",block:!0})]}),(0,s.jsx)("p",{className:"text-gray-700 dark:text-gray-300",children:"Where:"}),(0,s.jsxs)("ul",{className:"list-disc pl-6 space-y-1 text-gray-700 dark:text-gray-300 mt-2",children:[(0,s.jsxs)("li",{children:[(0,s.jsx)("strong",{children:"Q"})," (Query): What we're looking for"]}),(0,s.jsxs)("li",{children:[(0,s.jsx)("strong",{children:"K"})," (Key): What we match against"]}),(0,s.jsxs)("li",{children:[(0,s.jsx)("strong",{children:"V"})," (Value): What we retrieve"]}),(0,s.jsxs)("li",{children:[(0,s.jsxs)("strong",{children:["d",(0,s.jsx)("sub",{children:"k"})]}),": Dimensionality of the keys"]})]})]}),(0,s.jsx)(p,{isActive:"attention"===e})]})}),(0,s.jsx)("section",{className:"section-container",children:(0,s.jsxs)("div",{className:"max-w-4xl mx-auto",children:[(0,s.jsx)("h3",{className:"text-2xl font-bold mb-6",children:"Self-Attention: The Key Innovation"}),(0,s.jsxs)("p",{className:"mb-6 text-gray-700 dark:text-gray-300",children:["The transformer architecture introduces ",(0,s.jsx)("strong",{children:"self-attention"}),", where all queries, keys, and values come from the same sequence. This allows each token to attend to all positions in the sequence, facilitating the modeling of complex dependencies regardless of their distance."]}),(0,s.jsx)(f,{isActive:"attention"===e}),(0,s.jsxs)("div",{className:"mt-12",children:[(0,s.jsx)("h3",{className:"text-2xl font-bold mb-6",children:"Multi-Head Attention"}),(0,s.jsx)("p",{className:"mb-6 text-gray-700 dark:text-gray-300",children:"Rather than performing a single attention function, the transformer uses multiple attention heads in parallel. This allows the model to jointly attend to information from different representation subspaces at different positions."}),(0,s.jsxs)("div",{className:"formula mb-8",children:[(0,s.jsx)(m,{formula:"MultiHead(Q, K, V) = Concat(head_1, head_2, ..., head_h)W^O",block:!0}),(0,s.jsx)(m,{formula:"where\\; head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)",block:!0})]}),(0,s.jsx)(j,{isActive:"attention"===e})]})]})}),(0,s.jsx)("section",{ref:l.architecture,id:"architecture",className:"section-container bg-gray-50 dark:bg-slate-900",children:(0,s.jsxs)("div",{className:"max-w-4xl mx-auto",children:[(0,s.jsx)("h2",{className:"text-3xl font-bold mb-6",children:"The Transformer Architecture"}),(0,s.jsx)("p",{className:"mb-6 text-gray-700 dark:text-gray-300",children:"The complete transformer architecture consists of an encoder and decoder, each composed of multiple identical layers. The key innovation is replacing recurrence entirely with attention mechanisms and feed-forward networks."}),(0,s.jsxs)("div",{className:"card mb-8",children:[(0,s.jsx)("h3",{className:"text-xl font-semibold mb-4",children:"Key Components"}),(0,s.jsxs)("div",{className:"space-y-6",children:[(0,s.jsxs)("div",{children:[(0,s.jsx)("h4",{className:"font-medium mb-2",children:"1. Input Embedding"}),(0,s.jsxs)("p",{className:"text-gray-700 dark:text-gray-300",children:["Converts input tokens into dense vector representations of dimension d",(0,s.jsx)("sub",{children:"model"}),"."]})]}),(0,s.jsxs)("div",{children:[(0,s.jsx)("h4",{className:"font-medium mb-2",children:"2. Positional Encoding"}),(0,s.jsx)("p",{className:"text-gray-700 dark:text-gray-300 mb-3",children:"Since the transformer doesn't use recurrence or convolution, it has no inherent notion of token order. Positional encodings are added to the embeddings to inject information about the position of tokens in the sequence."}),(0,s.jsxs)("div",{className:"formula",children:[(0,s.jsx)(m,{formula:"PE_{(pos, 2i)} = sin(pos / 10000^{2i/d_{model}})",block:!0}),(0,s.jsx)(m,{formula:"PE_{(pos, 2i+1)} = cos(pos / 10000^{2i/d_{model}})",block:!0})]})]}),(0,s.jsxs)("div",{children:[(0,s.jsx)("h4",{className:"font-medium mb-2",children:"3. Multi-Head Attention"}),(0,s.jsx)("p",{className:"text-gray-700 dark:text-gray-300",children:"The transformer uses three types of attention:"}),(0,s.jsxs)("ul",{className:"list-disc pl-6 space-y-1 text-gray-700 dark:text-gray-300 mt-2",children:[(0,s.jsxs)("li",{children:[(0,s.jsx)("strong",{children:"Encoder Self-Attention:"})," Each encoder token attends to all encoder tokens"]}),(0,s.jsxs)("li",{children:[(0,s.jsx)("strong",{children:"Masked Decoder Self-Attention:"})," Each decoder token attends to all previous decoder tokens"]}),(0,s.jsxs)("li",{children:[(0,s.jsx)("strong",{children:"Encoder-Decoder Attention:"})," Each decoder token attends to all encoder tokens"]})]})]}),(0,s.jsxs)("div",{children:[(0,s.jsx)("h4",{className:"font-medium mb-2",children:"4. Feed-Forward Networks"}),(0,s.jsx)("p",{className:"text-gray-700 dark:text-gray-300 mb-3",children:"Each layer in both encoder and decoder contains a fully connected feed-forward network applied to each position separately and identically."}),(0,s.jsx)("div",{className:"formula",children:(0,s.jsx)(m,{formula:"FFN(x) = max(0, xW_1 + b_1)W_2 + b_2",block:!0})})]}),(0,s.jsxs)("div",{children:[(0,s.jsx)("h4",{className:"font-medium mb-2",children:"5. Residual Connections and Layer Normalization"}),(0,s.jsx)("p",{className:"text-gray-700 dark:text-gray-300",children:"Each sub-layer (attention and feed-forward) is wrapped with a residual connection followed by layer normalization:"}),(0,s.jsx)("div",{className:"formula mt-3",children:(0,s.jsx)(m,{formula:"LayerNorm(x + Sublayer(x))",block:!0})})]})]})]}),(0,s.jsxs)("div",{className:"mb-8",children:[(0,s.jsx)("h3",{className:"text-xl font-semibold mb-4",children:"Transformer Architecture Visualization"}),(0,s.jsx)("p",{className:"mb-4 text-gray-700 dark:text-gray-300",children:"Explore the transformer architecture step by step using the interactive visualization below:"}),(0,s.jsxs)("div",{className:"bg-white dark:bg-slate-800 p-6 rounded-lg shadow-md",children:[(0,s.jsxs)("div",{className:"flex items-center justify-between mb-4",children:[(0,s.jsx)("button",{onClick:()=>i(Math.max(0,a-1)),disabled:0===a,className:"px-3 py-1 rounded-md ".concat(0===a?"bg-gray-200 text-gray-500 cursor-not-allowed dark:bg-gray-700":"bg-gray-200 hover:bg-gray-300 dark:bg-gray-700 dark:hover:bg-gray-600"),children:"Previous"}),(0,s.jsxs)("span",{className:"text-sm",children:["Step ",a+1," of 7"]}),(0,s.jsx)("button",{onClick:()=>i(Math.min(6,a+1)),disabled:6===a,className:"px-3 py-1 rounded-md ".concat(6===a?"bg-gray-200 text-gray-500 cursor-not-allowed dark:bg-gray-700":"bg-indigo-100 hover:bg-indigo-200 text-indigo-800 dark:bg-indigo-900 dark:hover:bg-indigo-800 dark:text-indigo-200"),children:"Next"})]}),(0,s.jsx)(x,{activeStep:a})]})]}),(0,s.jsxs)("div",{className:"card",children:[(0,s.jsx)("h3",{className:"text-xl font-semibold mb-4",children:"Advantages Over Previous Architectures"}),(0,s.jsxs)("ul",{className:"list-disc pl-6 space-y-2 text-gray-700 dark:text-gray-300",children:[(0,s.jsxs)("li",{children:[(0,s.jsx)("strong",{children:"Parallelization:"})," Unlike RNNs, transformers can process all tokens in parallel, greatly speeding up training."]}),(0,s.jsxs)("li",{children:[(0,s.jsx)("strong",{children:"Long-range Dependencies:"})," The attention mechanism allows for direct connections between any two positions, making it easier to learn long-range dependencies."]}),(0,s.jsxs)("li",{children:[(0,s.jsx)("strong",{children:"No Information Bottleneck:"})," Information flows directly between all positions without being compressed into a fixed-size context vector."]}),(0,s.jsxs)("li",{children:[(0,s.jsx)("strong",{children:"Better Gradient Flow:"})," With direct connections between positions and residual connections throughout the network, gradients flow more easily during training."]})]})]})]})}),(0,s.jsx)("section",{ref:l.applications,id:"applications",className:"section-container",children:(0,s.jsxs)("div",{className:"max-w-4xl mx-auto",children:[(0,s.jsx)("h2",{className:"text-3xl font-bold mb-6",children:"Applications of Transformers"}),(0,s.jsx)("p",{className:"mb-8 text-gray-700 dark:text-gray-300",children:"The transformer architecture has revolutionized natural language processing and beyond, enabling breakthroughs in numerous domains."}),(0,s.jsxs)("div",{className:"grid grid-cols-1 md:grid-cols-2 gap-6 mb-12",children:[(0,s.jsxs)("div",{className:"card",children:[(0,s.jsx)("h3",{className:"text-xl font-semibold mb-3",children:"Natural Language Processing"}),(0,s.jsxs)("ul",{className:"list-disc pl-6 space-y-1 text-gray-700 dark:text-gray-300",children:[(0,s.jsx)("li",{children:"Machine Translation"}),(0,s.jsx)("li",{children:"Text Summarization"}),(0,s.jsx)("li",{children:"Question Answering"}),(0,s.jsx)("li",{children:"Text Generation"}),(0,s.jsx)("li",{children:"Sentiment Analysis"})]})]}),(0,s.jsxs)("div",{className:"card",children:[(0,s.jsx)("h3",{className:"text-xl font-semibold mb-3",children:"Computer Vision"}),(0,s.jsxs)("ul",{className:"list-disc pl-6 space-y-1 text-gray-700 dark:text-gray-300",children:[(0,s.jsx)("li",{children:"Image Classification"}),(0,s.jsx)("li",{children:"Object Detection"}),(0,s.jsx)("li",{children:"Image Segmentation"}),(0,s.jsx)("li",{children:"Image Generation"}),(0,s.jsx)("li",{children:"Video Understanding"})]})]}),(0,s.jsxs)("div",{className:"card",children:[(0,s.jsx)("h3",{className:"text-xl font-semibold mb-3",children:"Multimodal Learning"}),(0,s.jsxs)("ul",{className:"list-disc pl-6 space-y-1 text-gray-700 dark:text-gray-300",children:[(0,s.jsx)("li",{children:"Image Captioning"}),(0,s.jsx)("li",{children:"Visual Question Answering"}),(0,s.jsx)("li",{children:"Text-to-Image Generation"}),(0,s.jsx)("li",{children:"Audio-Text Understanding"})]})]}),(0,s.jsxs)("div",{className:"card",children:[(0,s.jsx)("h3",{className:"text-xl font-semibold mb-3",children:"Scientific Applications"}),(0,s.jsxs)("ul",{className:"list-disc pl-6 space-y-1 text-gray-700 dark:text-gray-300",children:[(0,s.jsx)("li",{children:"Protein Structure Prediction"}),(0,s.jsx)("li",{children:"Drug Discovery"}),(0,s.jsx)("li",{children:"Weather Forecasting"}),(0,s.jsx)("li",{children:"Chemical Reaction Prediction"})]})]})]}),(0,s.jsxs)("div",{className:"card",children:[(0,s.jsx)("h3",{className:"text-xl font-semibold mb-4",children:"Famous Transformer Models"}),(0,s.jsxs)("div",{className:"space-y-4",children:[(0,s.jsxs)("div",{children:[(0,s.jsx)("h4",{className:"font-medium",children:"BERT (Bidirectional Encoder Representations from Transformers)"}),(0,s.jsx)("p",{className:"text-sm text-gray-700 dark:text-gray-300",children:"Uses only the encoder portion of the transformer to create powerful contextual word embeddings."})]}),(0,s.jsxs)("div",{children:[(0,s.jsx)("h4",{className:"font-medium",children:"GPT (Generative Pre-trained Transformer)"}),(0,s.jsx)("p",{className:"text-sm text-gray-700 dark:text-gray-300",children:"Uses the decoder portion of the transformer for powerful autoregressive text generation."})]}),(0,s.jsxs)("div",{children:[(0,s.jsx)("h4",{className:"font-medium",children:"T5 (Text-to-Text Transfer Transformer)"}),(0,s.jsx)("p",{className:"text-sm text-gray-700 dark:text-gray-300",children:"Frames all NLP tasks as text-to-text problems, using the complete encoder-decoder transformer."})]}),(0,s.jsxs)("div",{children:[(0,s.jsx)("h4",{className:"font-medium",children:"Vision Transformer (ViT)"}),(0,s.jsx)("p",{className:"text-sm text-gray-700 dark:text-gray-300",children:"Adapts the transformer for image classification by treating image patches as tokens."})]})]})]})]})}),(0,s.jsx)("section",{ref:l.demo,id:"demo",className:"section-container bg-gray-50 dark:bg-slate-900",children:(0,s.jsxs)("div",{className:"max-w-4xl mx-auto",children:[(0,s.jsx)("h2",{className:"text-3xl font-bold mb-6",children:"Interactive Transformer Demo"}),(0,s.jsx)("p",{className:"mb-8 text-gray-700 dark:text-gray-300",children:"Experience the transformer in action with this interactive demo. Select a prompt and see how the model generates completions."}),(0,s.jsx)(k,{isActive:"demo"===e})]})}),(0,s.jsx)("footer",{className:"mt-16 py-8 bg-white dark:bg-slate-800 border-t border-gray-200 dark:border-gray-700",children:(0,s.jsx)("div",{className:"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8",children:(0,s.jsxs)("div",{className:"flex flex-col md:flex-row justify-between items-center",children:[(0,s.jsxs)("div",{className:"mb-4 md:mb-0",children:[(0,s.jsx)("h3",{className:"text-lg font-semibold mb-2",children:"Transformer Architecture Explained"}),(0,s.jsx)("p",{className:"text-sm text-gray-600 dark:text-gray-400",children:"An interactive visual journey through the transformer architecture"})]}),(0,s.jsxs)("div",{className:"flex flex-col items-center md:items-end",children:[(0,s.jsx)("p",{className:"text-sm text-gray-600 dark:text-gray-400 mb-2",children:'Based on the paper "Attention Is All You Need" by Vaswani et al.'}),(0,s.jsx)("a",{href:"https://arxiv.org/abs/1706.03762",target:"_blank",rel:"noopener noreferrer",className:"text-sm text-indigo-600 hover:text-indigo-800 dark:text-indigo-400 dark:hover:text-indigo-300",children:"Read the original paper →"})]})]})})})]})}},4036:(e,t,a)=>{Promise.resolve().then(a.bind(a,503))}},e=>{var t=t=>e(e.s=t);e.O(0,[562,330,559,441,684,358],()=>t(4036)),_N_E=e.O()}]);